{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing and mitigating discrimination in the COMPAS dataset\n",
    "\n",
    "Tutorial lab for the Hi! Paris 2023 summer school. \n",
    "\n",
    "\n",
    "## 0. Dataset loading and basic exploration\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "We will examine the ProPublica COMPAS dataset, which contains the records of all criminal defendants who were subject to COMPAS screening in Broward County, Florida, during 2013 and 2014. For each defendant, various information fields (‘features’) were gathered by ProPublica. Broadly, these fields are related to the defendant’s demographic information (e.g., gender and race), criminal history (e.g., the number of prior offenses) and administrative information about the case (e.g., the case number, arrest date). Finally, the dataset contains the risk of recidivism predicted by the COMPAS tool, and also information about whether the defendant did actually recidivate or not (ground truth label for us).\n",
    "\n",
    "The COMPAS score uses answers to 137 questions to assign a risk score to defendants -- essentially an estimate of the likelihood of re-arrest. The actual output is two-fold: a risk rating of 1-10 and a \"low\", \"medium\", or \"high\" risk label.\n",
    "\n",
    "Link to dataset: https://github.com/propublica/compas-analysis\n",
    "\n",
    "The file we will analyze is: compas-scores-two-years.csv\n",
    "\n",
    "Link to the ProPublica article:\n",
    "\n",
    "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n",
    "Some of the code below is adapted from the Propublica github repository above, and from\n",
    "\n",
    "https://investigate.ai/propublica-criminal-sentencing/week-5-1-machine-bias-class/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "We first need to load the data from the ProPublica repo:\n",
    "https://github.com/propublica/compas-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import urllib.request\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\") # get the current directory listing\n",
    "    print(\"Looking for file '%s' in the current directory...\",fname)\n",
    "\n",
    "    if fname not in files:\n",
    "        print(\"'%s' not found! Downloading from GitHub...\",fname)\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        response = urllib.request.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print(\"'%s' download and saved locally..\",fname)\n",
    "    else:\n",
    "        print(\"File found in current directory..\")\n",
    "    \n",
    "COMPAS_INPUT_FILE = \"compas-scores-two-years.csv\"\n",
    "check_data_file(COMPAS_INPUT_FILE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning data\n",
    "\n",
    "The following code loads the data using pandas and cleans it according to ProPublica's cleaning: \n",
    "\n",
    "\"\n",
    "If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "\n",
    "We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "\n",
    "In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed\n",
    "\n",
    "We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\" \n",
    "\"\n",
    "\n",
    "Finally, it converts the data to a dictionary with np arrays, which will be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file\n",
    "df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "# print(df.shape)\n",
    "\n",
    "df = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
    "\n",
    "df = df[\n",
    "    (df.days_b_screening_arrest <= 30) &  \n",
    "    (df.days_b_screening_arrest >= -30) &  \n",
    "    (df.is_recid != -1) &\n",
    "    (df.c_charge_degree != 'O') &\n",
    "    (df.score_text != 'N/A')\n",
    "]\n",
    "\n",
    "df.reset_index(inplace=True, drop=True) # renumber the rows from 0 again\n",
    "# df.shape\n",
    "\n",
    "# Conversion to dictionary with np arrays\n",
    "data = df.to_dict('list')\n",
    "for k in data.keys():\n",
    "    data[k] = np.array(data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset\n",
    "\n",
    "The following pandas commands are very convenient to explore the data. Uncomment them to explore the data. \n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Add a command to print the number of defendents per race. \n",
    "\n",
    "In the following, we will look mostly at African-Americans and Caucasians(termed blacks and whites for short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head() # check some examples of data\n",
    "# df.columns # prints the features\n",
    "# df.isnull().sum() #check for missing values\n",
    "# df.describe() # generates descriptive statistics (e.g., to check for outliers)\n",
    "# df.race.unique() # different races we have in the dataset\n",
    "# df.age_cat.value_counts() #number of people by age category\n",
    "# df.score_text.value_counts() #number of people by COMPAS risk category\n",
    "\n",
    "# TODO: Insert your code below this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analysis of the COMPAS scores: basic exploration and fairness metrics \n",
    "\n",
    "### Basic analysis of the bias in COMPAS scores\n",
    "\n",
    "We now look at the COMPAS scores (deciles first, then text scores) as a function of the sensitive attribute (race or gender) to observe potential differences. \n",
    "\n",
    "We start by observing the scores for different groupes. \n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Plot the histograms of decile scores for Black and White defendant and observe the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of decile scores for Blacks (blue) and Whites (orange)\n",
    "# TODO: Insert your code below this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above observation can be explained by a dependence between the race and true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recidivism rates by race\n",
    "recid_race = pd.crosstab(df.race, df.two_year_recid)\n",
    "recid_race['rate'] = recid_race[1] / recid_race.sum(axis=1)\n",
    "recid_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at whether we observe a similar phenomenon on the text scores of COMPAS (low, medium, high risk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high risk rates by race\n",
    "score_race = pd.crosstab(df.race, df.score_text)\n",
    "score_race['High risk rate'] = score_race['High'] / score_race.sum(axis=1)\n",
    "score_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness metrics for the COMPAS scores\n",
    "\n",
    "We do not have the actual scores that are used to compte text scores Low-Med-High; hence we cannot investigate directly the calibration. However, we can use the decile score as a proxy, and we can investigate PPV. \n",
    "\n",
    "Let us first plot the probability of recidivism by decile score. We observe that it is not very far from a diagonale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of recidivism by decile\n",
    "df.groupby(df.decile_score).mean(numeric_only=True)['two_year_recid'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO</span>: Plot a similar graph with separated bars for Blacks and Whites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of recidivism by decile and race\n",
    "# TODO: Insert your code below this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze the COMPAS score as a classifier, we transform it into a binary outcome by splitting \"low\" (class 0) from \"medium or high\" risk (class 1). We can then compute standard quantities such as the confusion matrix or PPV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPAS recidivism confusion matrix\n",
    "df['guessed_recid'] = df.score_text != 'Low'\n",
    "df['actual_recid'] = df.two_year_recid == 1\n",
    "cm = pd.crosstab(df.actual_recid, df.guessed_recid)\n",
    "cm # for \"confusion matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally compute the PPV, TPR and FPR of the COMPAS classifier. \n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Complete the print_ppv_fpv function. Observe the differences in the metrics between Blacks and Whites and comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm is a confusion matrix. The rows are guessed, the columns are actual \n",
    "def print_ppv_fpv(cm):\n",
    "    # the indices here are [col][row] or [actual][guessed]\n",
    "    TN = cm[False][False]   \n",
    "    TP = cm[True][True]\n",
    "    FN = cm[True][False]\n",
    "    FP = cm[False][True]\n",
    "# TODO: Uncomment the lines below and replace the ... by the appropriate expressions of TN, TP, FN, FP  \n",
    "#     print('Accuracy: ', ...)\n",
    "#     print('PPV: ', ...)\n",
    "#     print('FPR: ', ...)\n",
    "#     print('FNR: ', ...)\n",
    "#     print()\n",
    "#\n",
    "\n",
    "def print_metrics(guessed, actual):\n",
    "    cm = pd.crosstab(guessed, actual, rownames=['guessed'], colnames=['actual'])\n",
    "    print(cm)\n",
    "    print()\n",
    "    print_ppv_fpv(cm)   \n",
    "    \n",
    "print('White')\n",
    "subset = df[df.race == 'Caucasian']\n",
    "print_metrics(subset.guessed_recid, subset.actual_recid)\n",
    "\n",
    "print('Black')\n",
    "subset = df[df.race == 'African-American']\n",
    "print_metrics(subset.guessed_recid, subset.actual_recid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the False Positive Rate is substantially higher for black defendants, but the PPV is similar between blacks and whites. That is, the COMPAS score satisfies sufficiency, but not separation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring and mitigating bias in a custom classifier\n",
    "\n",
    "### Training a classifier from the ground truth label\n",
    "\n",
    "We now train a classifier (a simple logistic regression) on the label two_year_recid. We work on a subset of features: [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"]. \n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Complete the model definition and fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"] #features to be used for classification\n",
    "CONT_VARIABLES = [\"priors_count\"] # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "CLASS_FEATURE = \"two_year_recid\" # the decision variable\n",
    "SENSITIVE_ATTRS = [\"race\"]\n",
    "\n",
    "\n",
    "y = data[CLASS_FEATURE]\n",
    "X = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\n",
    "\n",
    "for attr in FEATURES_CLASSIFICATION:\n",
    "    vals = data[attr]\n",
    "    if attr in CONT_VARIABLES:\n",
    "        vals = [float(v) for v in vals]\n",
    "        vals = preprocessing.scale(vals) # 0 mean and 1 variance  \n",
    "        vals = np.reshape(vals, (len(y), -1)) # convert from 1-d arr to a 2-d arr with one col\n",
    "\n",
    "    else: # this encodes categorical variables in a numerical way -- there are other ways to do it\n",
    "        enc = preprocessing.LabelBinarizer()\n",
    "        enc.fit(vals)\n",
    "        vals = enc.transform(vals)\n",
    "\n",
    "    # add to learnable features\n",
    "    X = np.hstack((X, vals))\n",
    "\n",
    "    \n",
    "# the following is a way to keep track of the race after the train_test_split -- there are other ways to do it    \n",
    "ind = data[\"race\"]==\"African-American\"\n",
    "X_b = X[ind] \n",
    "y_b = y[ind]\n",
    "ind = data[\"race\"]==\"Caucasian\"\n",
    "X_w = X[ind]        \n",
    "y_w = y[ind]\n",
    "ind = [data[\"race\"][i]!=\"Caucasian\" and data[\"race\"][i]!=\"African-American\" for i in range(len(y))]\n",
    "X_n = X[ind]        \n",
    "y_n = y[ind]\n",
    "    \n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.3, random_state=1234)\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.3, random_state=5678)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_n, y_n, test_size=0.3, random_state=9012)\n",
    "\n",
    "X_train = np.vstack((X_train_b, X_train_w, X_train_n))\n",
    "y_train = np.hstack((y_train_b, y_train_w, y_train_n))\n",
    "X_test = np.vstack((X_test_b, X_test_w, X_test_n))\n",
    "y_test = np.hstack((y_test_b, y_test_w, y_test_n))\n",
    "\n",
    "\n",
    "# TODO: Uncomment and complete the code below\n",
    "# model = ...\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the model accuracy. Compare to the COMPAS accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the ROC curve for the model on the global population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "scores = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, scores[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.title('ROC overall population')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO</span>: Plot the ROC curve for Blacks and Whites in two separate curves (in the same plot). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert your code below this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring fairness in the trained classifier\n",
    "\n",
    "We can now check the fairness of this simple logistic regression, when choosing an arbitrary threshold common to the two groups. \n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Compute the predictions for a common threshold (threshold_common), then compute the PPV, TPR, FPR (you can use the print_metrics function from above). Comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_common = 0.5\n",
    "# TODO: Insert your code below this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also investigate the calibration more finely using the calibration module from sklearn. \n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Complete the code below to plot the calibration curve for the whole population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "# TODO: Uncomment and complete the code below\n",
    "# scores = ...\n",
    "# prob_true, prob_pred = ...\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# only this line is calibration curves\n",
    "plt.plot(prob_pred,prob_true, marker='o', linewidth=1, label='all')\n",
    "\n",
    "# reference line, legends, and axis labels\n",
    "line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "transform = ax.transAxes\n",
    "line.set_transform(transform)\n",
    "ax.add_line(line)\n",
    "fig.suptitle('Calibration plot for the logistic regression (overall)')\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('True probability in each bin')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO</span>: Complete the code below to plot the calibration curve for the blacks and white separately. Conclude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and complete the code below\n",
    "# ...\n",
    "# prob_true_b, prob_pred_b = ...\n",
    "# ...\n",
    "# prob_true_w, prob_pred_w = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# only this line is calibration curves\n",
    "plt.plot(prob_pred_b,prob_true_b, marker='o', linewidth=1, label='Blacks')\n",
    "plt.plot(prob_pred_w,prob_true_w, marker='o', linewidth=1, label='Whites')\n",
    "\n",
    "# reference line, legends, and axis labels\n",
    "line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "transform = ax.transAxes\n",
    "line.set_transform(transform)\n",
    "ax.add_line(line)\n",
    "fig.suptitle('Calibration plot for the logistic regression (overall)')\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('True probability in each bin')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigating via post-processing for equal opportunity\n",
    "\n",
    "Using the logistic regression classifier trained above, we now compute group-dependent thresholds to achieve equal opportunity\n",
    "\n",
    "<span style=\"color:red\">TODO</span>: Compute the thresholds for the black and white groups to achieve a given target TPR (say, 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tpr = 0.2\n",
    "\n",
    "# TODO: Insert your code below this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO</span>: Compute the performance metrics of the classifier with group-dependent threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert your code below this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Because the ROC curves are the same for blacks and whites, we get equalized odds for free when imposing equal opportunity. However, we lose sufficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Going beyond with fairlearn\n",
    "\n",
    "The fairlearn package (see documentation at https://fairlearn.org/) implements a number of methods to automatically compute fair classifiers. We only very briefly illustrate it here with one in-processing method (exponentiated gradient). \n",
    "\n",
    "If you have not already installed it, you will need to install fairlearn with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train the mitigated classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, TruePositiveRateParity\n",
    "\n",
    "constraint = TruePositiveRateParity()\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "\n",
    "sensitive_features_train = np.hstack( (([\"Black\"]*len(y_train_b)), ([\"White\"]*len(y_train_w)), ([\"Other\"]*len(y_train_n))))\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe the effect of mitigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_b = classifier.predict(X_test_b)\n",
    "y_pred_w = classifier.predict(X_test_w)\n",
    "\n",
    "print('White, before mitigation')\n",
    "print_metrics(y_pred_w.astype(bool), y_test_w.astype(bool))\n",
    "\n",
    "print('Black, before mitigation')\n",
    "print_metrics(y_pred_b.astype(bool), y_test_b.astype(bool))\n",
    "\n",
    "y_pred_mitigated_b = mitigator.predict(X_test_b)\n",
    "y_pred_mitigated_w = mitigator.predict(X_test_w)\n",
    "\n",
    "print('White, after mitigation')\n",
    "print_metrics(y_pred_mitigated_w.astype(bool), y_test_w.astype(bool))\n",
    "\n",
    "print('Black, after mitigation')\n",
    "print_metrics(y_pred_mitigated_b.astype(bool), y_test_b.astype(bool))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: fairlearn also provides functionalities to compute the metrics by group. The code below gives an example of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import TruePositiveRateParity\n",
    "from fairlearn.metrics import true_positive_rate, MetricFrame\n",
    "\n",
    "tprp = TruePositiveRateParity(difference_bound=0.01)\n",
    "\n",
    "X = X_test\n",
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "sensitive_features = np.hstack( (np.array([\"Black\"]*len(y_test_b)), np.array([\"White\"]*len(y_test_w)), np.array([\"Other\"]*len(y_test_n))))\n",
    "\n",
    "\n",
    "tpr_summary = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_true,\n",
    "                          y_pred=y_pred,\n",
    "                          sensitive_features=sensitive_features)\n",
    "tpr_summary.by_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mitigated = mitigator.predict(X)\n",
    "\n",
    "tpr_summary = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_true,\n",
    "                          y_pred=y_pred_mitigated,\n",
    "                          sensitive_features=sensitive_features)\n",
    "tpr_summary.overall\n",
    "tpr_summary.by_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
